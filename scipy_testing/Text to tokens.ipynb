{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\andraszeka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\andraszeka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\andraszeka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\andraszeka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Kogan North-19 was completed by Arrow Energy NL in June 2005. The main purpose of the well was as a production \n",
    "well for the Kogan North Project.\"\"\" #Kogan North-19 reached a depth of 242.42m. Surface casing was set to 108.30m. To avoid \n",
    "#formation damage and increase production rates, the coals over the production interval were under reamed to 16” using air \n",
    "#and foam. The well was completed with slotted and blank 6 5/8” casing with the annulus gravel packed. The drilling and \n",
    "#completion operation took a total of 19 days from spud to release. The drilling of the hole was delayed considerably by \n",
    "#both the weather producing conditions too wet to drill ahead or set up in, and numerous days waiting on parts to repair \n",
    "#the mud pump. Delays were also experienced waiting for the wireline logging contractor. Excessive water influx was \n",
    "#encountered in drilling the production section of the hole.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kogan',\n",
       " 'North-19',\n",
       " 'was',\n",
       " 'completed',\n",
       " 'by',\n",
       " 'Arrow',\n",
       " 'Energy',\n",
       " 'NL',\n",
       " 'in',\n",
       " 'June',\n",
       " '2005',\n",
       " '.',\n",
       " 'The',\n",
       " 'main',\n",
       " 'purpose',\n",
       " 'of',\n",
       " 'the',\n",
       " 'well',\n",
       " 'was',\n",
       " 'as',\n",
       " 'a',\n",
       " 'production',\n",
       " 'well',\n",
       " 'for',\n",
       " 'the',\n",
       " 'Kogan',\n",
       " 'North',\n",
       " 'Project',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Kogan/NNP)\n",
      "  North-19/NNP\n",
      "  was/VBD\n",
      "  completed/VBN\n",
      "  by/IN\n",
      "  (PERSON Arrow/NNP Energy/NNP NL/NNP)\n",
      "  in/IN\n",
      "  June/NNP\n",
      "  2005/CD\n",
      "  ./.\n",
      "  The/DT\n",
      "  main/JJ\n",
      "  purpose/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  well/NN\n",
      "  was/VBD\n",
      "  as/IN\n",
      "  a/DT\n",
      "  production/NN\n",
      "  well/RB\n",
      "  for/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION Kogan/NNP North/NNP Project/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokens)\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\andraszeka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "nltk.download('treebank')\n",
    "t = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
    "t.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CookieJar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-ca0ead15a43e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCookieJar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murllib2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTTPCookieProcessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'User-agent'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Mozilla/5.0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CookieJar' is not defined"
     ]
    }
   ],
   "source": [
    "cj = CookieJar()\n",
    "opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (Chunk incredibly/RB intimidating/VBG NLP/NNP)\n",
      "  scares/NNS\n",
      "  people/NNS\n",
      "  away/RB\n",
      "  who/WP\n",
      "  are/VBP\n",
      "  sissies/NNS)\n"
     ]
    }
   ],
   "source": [
    "exampleArray = ['The incredibly intimidating NLP scares people away who are sissies']\n",
    "\n",
    "def processContent():\n",
    "    try:\n",
    "        for item in exampleArray:\n",
    "            tokenized = nltk.word_tokenize(item)\n",
    "            tagged = nltk.pos_tag(tokenized)\n",
    "\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            print(chunked)\n",
    "            chunked.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "    \n",
    "\n",
    "\n",
    "processContent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Starbucks', 'NNS'), ('is', 'VBZ'), ('not', 'RB'), ('doing', 'VBG'), ('very', 'RB'), ('well', 'RB'), ('lately', 'RB'), ('.', '.')]\n",
      "<class 'Exception'>\n"
     ]
    }
   ],
   "source": [
    "contentArray =['Starbucks is not doing very well lately.',\n",
    "               'Overall, while it may seem there is already a Starbucks on every corner, Starbucks still has a lot of room to grow.',\n",
    "               'They just began expansion into food products, which has been going quite well so far for them.',\n",
    "               'I can attest that my own expenditure when going to Starbucks has increased, in lieu of these food products.',\n",
    "               'Starbucks is also indeed expanding their number of stores as well.',\n",
    "               'Starbucks still sees strong sales growth here in the united states, and intends to actually continue increasing this.',\n",
    "               'Starbucks also has one of the more successful loyalty programs, which accounts for 30%  of all transactions being loyalty-program-based.',\n",
    "               'As if news could not get any more positive for the company, Brazilian weather has become ideal for producing coffee beans.',\n",
    "               'Brazil is the world\\'s #1 coffee producer, the source of about 1/3rd of the entire world\\'s supply!',\n",
    "               'Given the dry weather, coffee farmers have amped up production, to take as much of an advantage as possible with the dry weather.',\n",
    "               'Increase in supply... well you know the rules...',]\n",
    "\n",
    "def processLanguage():\n",
    "    try:\n",
    "        for item in contentArray:\n",
    "            tokenized = nltk.word_tokenize(item)\n",
    "            tagged = nltk.pos_tag(tokenized)\n",
    "            print(tagged)\n",
    "\n",
    "            namedEnt = nltk.ne_chunk(tagged)\n",
    "            namedEnt.draw()\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "    except Exception:\n",
    "        print(str(Exception))\n",
    "        \n",
    "\n",
    "processLanguage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "input_path = 'entity-annotated-corpus/ner_dataset.tsv'\n",
    "out_path = 'entity-annotated-corpus/ner_corpus_260.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32m<ipython-input-2-3cebe9a9e94d>\u001b[0m(64)\u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m     60 \u001b[1;33m    \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     61 \u001b[1;33m        \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unable to process file\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"error = \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     62 \u001b[1;33m        \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     63 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m---> 64 \u001b[1;33m\u001b[0mtsv_to_json_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'abc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n"
     ]
    }
   ],
   "source": [
    "%debug\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "def tsv_to_json_format(input_path,output_path,unknown_label):\n",
    "    try:\n",
    "        f=open(input_path,'r') # input file\n",
    "        fp=open(output_path, 'w') # output file\n",
    "        data_dict={}\n",
    "        annotations =[]\n",
    "        label_dict={}\n",
    "        s=''\n",
    "        start=0\n",
    "        for line in f:\n",
    "            if line[:-1]!='.\\tO':\n",
    "                word,entity=line.split('\\t')\n",
    "                s+=word+\" \"\n",
    "                entity=entity[:-1]\n",
    "                if entity!=unknown_label:\n",
    "                    if len(entity) != 1:\n",
    "                        d={}\n",
    "                        d['text']=word\n",
    "                        d['start']=start\n",
    "                        d['end']=start+len(word)-1  \n",
    "                        try:\n",
    "                            label_dict[entity].append(d)\n",
    "                        except:\n",
    "                            label_dict[entity]=[]\n",
    "                            label_dict[entity].append(d) \n",
    "                start+=len(word)+1\n",
    "            else:\n",
    "                data_dict['content']=s\n",
    "                s=''\n",
    "                label_list=[]\n",
    "                for ents in list(label_dict.keys()):\n",
    "                    for i in range(len(label_dict[ents])):\n",
    "                        if(label_dict[ents][i]['text']!=''):\n",
    "                            l=[ents,label_dict[ents][i]]\n",
    "                            for j in range(i+1,len(label_dict[ents])): \n",
    "                                if(label_dict[ents][i]['text']==label_dict[ents][j]['text']):  \n",
    "                                    di={}\n",
    "                                    di['start']=label_dict[ents][j]['start']\n",
    "                                    di['end']=label_dict[ents][j]['end']\n",
    "                                    di['text']=label_dict[ents][i]['text']\n",
    "                                    l.append(di)\n",
    "                                    label_dict[ents][j]['text']=''\n",
    "                            label_list.append(l)                          \n",
    "                            \n",
    "                for entities in label_list:\n",
    "                    label={}\n",
    "                    label['label']=[entities[0]]\n",
    "                    label['points']=entities[1:]\n",
    "                    annotations.append(label)\n",
    "                data_dict['annotation']=annotations\n",
    "                annotations=[]\n",
    "                json.dump(data_dict, fp)\n",
    "                fp.write('\\n')\n",
    "                data_dict={}\n",
    "                start=0\n",
    "                label_dict={}\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process file\" + \"\\n\" + \"error = \" + str(e))\n",
    "        return None\n",
    "\n",
    "tsv_to_json_format(input_path, out_path, 'abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
